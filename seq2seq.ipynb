{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMo2LR6UIgnrO4KGsu5GetE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sotetsuk/LectureColab/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTpJQiNtADt3",
        "outputId": "4b27dfe5-fc47-4ae5-b45a-b39ab5de511d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'small_parallel_enja'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 35 (from 1)\u001b[K\n",
            "Receiving objects: 100% (35/35), 1.37 MiB | 9.15 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/odashi/small_parallel_enja.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "if \"KERAS_BACKEND\" not in os.environ:\n",
        "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import keras\n",
        "from keras import layers, ops\n",
        "\n",
        "\n",
        "class LuongAttention(layers.Layer):\n",
        "    def call(self, inputs, mask=None):\n",
        "        query, value = inputs\n",
        "        scores = ops.matmul(query, ops.transpose(value, [0, 2, 1]))\n",
        "\n",
        "        if mask is not None:\n",
        "            enc_mask = mask[1] if isinstance(mask, (list, tuple)) else mask\n",
        "            if enc_mask is not None:\n",
        "                enc_mask = ops.expand_dims(ops.cast(enc_mask, scores.dtype), axis=1)\n",
        "                scores = scores - 1e9 * (1.0 - enc_mask)\n",
        "\n",
        "        weights = ops.softmax(scores, axis=-1)\n",
        "        return ops.matmul(weights, value)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask[0] if isinstance(mask, (list, tuple)) and mask else mask\n",
        "\n",
        "\n",
        "class TranslationDataset(keras.utils.PyDataset):\n",
        "    def __init__(self, en_sentences, ja_sentences, en_vectorizer, ja_vectorizer,\n",
        "                 batch_size=32, shuffle=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.en_sequences = ops.convert_to_tensor(en_vectorizer(en_sentences), dtype=\"int32\")\n",
        "        self.ja_sequences_input = ops.convert_to_tensor(\n",
        "            ja_vectorizer([\"<s> \" + s for s in ja_sentences]), dtype=\"int32\")\n",
        "        self.ja_sequences_target = ops.convert_to_tensor(\n",
        "            ja_vectorizer([s + \" </s>\" for s in ja_sentences]), dtype=\"int32\")\n",
        "\n",
        "        self.indices = ops.arange(len(en_sentences))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        return {\"encoder_input\": self.en_sequences[batch_indices],\n",
        "                \"decoder_input\": self.ja_sequences_input[batch_indices]}, \\\n",
        "               self.ja_sequences_target[batch_indices]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.indices = ops.convert_to_tensor(keras.random.shuffle(self.indices))\n",
        "\n",
        "\n",
        "def translate_sentences(model, en_sentences, en_vectorizer, ja_vectorizer, max_length=20):\n",
        "    en_sequences = ops.convert_to_tensor(en_vectorizer(en_sentences), dtype=\"int32\")\n",
        "    start_token_index = int(ja_vectorizer([\"<s>\"])[0][0])\n",
        "    end_token_index = int(ja_vectorizer([\"</s>\"])[0][0])\n",
        "    ja_vocab = ja_vectorizer.get_vocabulary()\n",
        "\n",
        "    translations = []\n",
        "    for i in range(len(en_sentences)):\n",
        "        current_sequence = [start_token_index]\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            enc_input = ops.expand_dims(en_sequences[i], axis=0)\n",
        "            dec_input = ops.convert_to_tensor([current_sequence], dtype=\"int32\")\n",
        "            predictions = model.predict([enc_input, dec_input], verbose=0)\n",
        "            next_token = int(ops.argmax(predictions[0, len(current_sequence)-1, :]))\n",
        "\n",
        "            if next_token == end_token_index:\n",
        "                break\n",
        "            current_sequence.append(next_token)\n",
        "\n",
        "        translated_tokens = [ja_vocab[idx] for idx in current_sequence[1:]\n",
        "                           if idx < len(ja_vocab) and ja_vocab[idx] not in [\"\", \"</s>\"]]\n",
        "        translations.append(\" \".join(translated_tokens))\n",
        "\n",
        "    return translations\n",
        "\n",
        "\n",
        "def masked_sparse_crossentropy(y_true, y_pred):\n",
        "    loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "    mask = ops.cast(ops.not_equal(y_true, 0), dtype=loss.dtype)\n",
        "    loss = loss * mask\n",
        "    return ops.sum(loss) / ops.sum(mask + keras.backend.epsilon())\n",
        "\n",
        "\n",
        "def build_seq2seq_model(enc_vocab_size, dec_vocab_size, embedding_dim=256, latent_dim=512, num_layers=2):\n",
        "    enc_in = keras.Input(shape=(None,), name=\"encoder_input\")\n",
        "    x = layers.Embedding(enc_vocab_size, embedding_dim, mask_zero=True)(enc_in)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    enc_states = []\n",
        "    for i in range(num_layers):\n",
        "        x, h, c = layers.LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "                              dropout=0.3, name=f\"enc_lstm_{i+1}\")(x)\n",
        "        enc_states.append([h, c])\n",
        "    enc_out = x\n",
        "\n",
        "    dec_in = keras.Input(shape=(None,), name=\"decoder_input\")\n",
        "    y = layers.Embedding(dec_vocab_size, embedding_dim, mask_zero=True)(dec_in)\n",
        "    y = layers.Dropout(0.3)(y)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        y = layers.LSTM(latent_dim, return_sequences=True, dropout=0.3,\n",
        "                        name=f\"dec_lstm_{i+1}\")(y, initial_state=enc_states[i])\n",
        "\n",
        "    context = LuongAttention(name=\"luong_attention\")([y, enc_out])\n",
        "    y = layers.Concatenate(axis=-1)([y, context])\n",
        "    out = layers.Dense(dec_vocab_size, activation=None, name=\"output\")(y)\n",
        "\n",
        "    return keras.Model([enc_in, dec_in], out)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(f\"Keras {keras.__version__}, Backend: {keras.config.backend()}\")\n",
        "\n",
        "    # Load data\n",
        "    def load_data(split):\n",
        "        with open(f\"small_parallel_enja/{split}.en\", \"r\") as f:\n",
        "            en = [line.strip() for line in f]\n",
        "        with open(f\"small_parallel_enja/{split}.ja\", \"r\") as f:\n",
        "            ja = [line.strip() for line in f]\n",
        "        return en, ja\n",
        "\n",
        "    train_en, train_ja = load_data(\"train\")\n",
        "    val_en, val_ja = load_data(\"dev\")\n",
        "\n",
        "    # Create vectorizers\n",
        "    en_vectorizer = layers.TextVectorization(\n",
        "        max_tokens=5000, output_mode=\"int\", output_sequence_length=20,\n",
        "        standardize=\"lower_and_strip_punctuation\")\n",
        "    ja_vectorizer = layers.TextVectorization(\n",
        "        max_tokens=5000, output_mode=\"int\", output_sequence_length=20,\n",
        "        standardize=None, split=\"whitespace\")\n",
        "\n",
        "    en_vectorizer.adapt(train_en)\n",
        "    ja_vectorizer.adapt([\"<s>\", \"</s>\"] + train_ja +\n",
        "                        [\"<s> \" + s + \" </s>\" for s in train_ja[:1000]])\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TranslationDataset(train_en, train_ja, en_vectorizer, ja_vectorizer, 64, True)\n",
        "    val_dataset = TranslationDataset(val_en, val_ja, en_vectorizer, ja_vectorizer, 64, False)\n",
        "\n",
        "    # Build and compile model\n",
        "    model = build_seq2seq_model(len(en_vectorizer.get_vocabulary()),\n",
        "                                len(ja_vectorizer.get_vocabulary()))\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
        "                  loss=masked_sparse_crossentropy, metrics=[\"accuracy\"])\n",
        "\n",
        "    # Sample sentences for evaluation\n",
        "    sample_sentences = val_en[:5]\n",
        "\n",
        "    # Translation callback\n",
        "    class TranslationCallback(keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            if epoch % 5 == 0:  # Print every 5 epochs\n",
        "                print(f\"\\nEpoch {epoch + 1} translations:\")\n",
        "                translations = translate_sentences(self.model, sample_sentences,\n",
        "                                                 en_vectorizer, ja_vectorizer)\n",
        "                for en, ja in zip(sample_sentences[:5], translations[:5]):  # Show only 2\n",
        "                    print(f\"EN: {en}\\nJA: {ja}\\n\")\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=50,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[\n",
        "            keras.callbacks.ModelCheckpoint(\"seq2seq_model.keras\", save_best_only=True,\n",
        "                                          monitor=\"val_loss\"),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n",
        "                                            patience=2, min_lr=1e-6),\n",
        "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10,\n",
        "                                        restore_best_weights=True),\n",
        "            TranslationCallback()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining complete! Best val_loss: {min(history.history['val_loss']):.4f}\")"
      ],
      "metadata": {
        "id": "1Z0E8PKmAEUA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdR7TRXXAQrX",
        "outputId": "30dcfe2b-e933-4c76-85b7-6e7919f9ee4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras 3.8.0, Backend: torch\n",
            "Epoch 1/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.1555 - loss: 4.3049\n",
            "Epoch 1 translations:\n",
            "EN: show your own business .\n",
            "JA: 私 は 何 を し て くださ い 。\n",
            "\n",
            "EN: he lived a hard life .\n",
            "JA: 彼 は その 仕事 を し た 。\n",
            "\n",
            "EN: no . i 'm sorry , i 've got to go back early .\n",
            "JA: 君 は その 仕事 を 見 て い る の は な い 。\n",
            "\n",
            "EN: she wrote to me to come at once .\n",
            "JA: 彼女 は 私 の ため に [UNK] し た 。\n",
            "\n",
            "EN: i can 't swim at all .\n",
            "JA: 私 は その 仕事 を し て い る 。\n",
            "\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 166ms/step - accuracy: 0.1555 - loss: 4.3040 - val_accuracy: 0.2830 - val_loss: 2.8162 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 150ms/step - accuracy: 0.2884 - loss: 2.7232 - val_accuracy: 0.3224 - val_loss: 2.3553 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 148ms/step - accuracy: 0.3290 - loss: 2.2358 - val_accuracy: 0.3596 - val_loss: 1.9549 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 148ms/step - accuracy: 0.3637 - loss: 1.8235 - val_accuracy: 0.3829 - val_loss: 1.7402 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 151ms/step - accuracy: 0.3907 - loss: 1.5449 - val_accuracy: 0.3951 - val_loss: 1.6276 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.4103 - loss: 1.3551\n",
            "Epoch 6 translations:\n",
            "EN: show your own business .\n",
            "JA: 仕事 を 持 っ て くださ い 。\n",
            "\n",
            "EN: he lived a hard life .\n",
            "JA: 彼 は [UNK] の 生活 を し て い た 。\n",
            "\n",
            "EN: no . i 'm sorry , i 've got to go back early .\n",
            "JA: こんな に 早 く 帰 り ま せ ん 。\n",
            "\n",
            "EN: she wrote to me to come at once .\n",
            "JA: 彼女 は 私 に すぐ に 返事 を かけ て くれ た 。\n",
            "\n",
            "EN: i can 't swim at all .\n",
            "JA: 私 は まったく 泳げ な い 。\n",
            "\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 156ms/step - accuracy: 0.4103 - loss: 1.3551 - val_accuracy: 0.4032 - val_loss: 1.5480 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 150ms/step - accuracy: 0.4267 - loss: 1.2066 - val_accuracy: 0.4117 - val_loss: 1.5002 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 151ms/step - accuracy: 0.4410 - loss: 1.0849 - val_accuracy: 0.4107 - val_loss: 1.4882 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 151ms/step - accuracy: 0.4525 - loss: 0.9979 - val_accuracy: 0.4195 - val_loss: 1.4563 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 149ms/step - accuracy: 0.4641 - loss: 0.9167 - val_accuracy: 0.4175 - val_loss: 1.4765 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.4729 - loss: 0.8499\n",
            "Epoch 11 translations:\n",
            "EN: show your own business .\n",
            "JA: 自分 の 仕事 を 持 っ て くださ い 。\n",
            "\n",
            "EN: he lived a hard life .\n",
            "JA: 彼 は 辛 い 生活 を し た 。\n",
            "\n",
            "EN: no . i 'm sorry , i 've got to go back early .\n",
            "JA: いいえ 、 もう 帰 っ て き た かっ た の で す 。\n",
            "\n",
            "EN: she wrote to me to come at once .\n",
            "JA: 彼女 は すぐ に 私 に 手紙 を くれ と くれ た 。\n",
            "\n",
            "EN: i can 't swim at all .\n",
            "JA: 私 は まったく 泳げ な い 。\n",
            "\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 151ms/step - accuracy: 0.4729 - loss: 0.8499 - val_accuracy: 0.4227 - val_loss: 1.4728 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 149ms/step - accuracy: 0.4880 - loss: 0.7511 - val_accuracy: 0.4263 - val_loss: 1.4520 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 149ms/step - accuracy: 0.4968 - loss: 0.6853 - val_accuracy: 0.4286 - val_loss: 1.4591 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 149ms/step - accuracy: 0.5027 - loss: 0.6497 - val_accuracy: 0.4258 - val_loss: 1.4626 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 148ms/step - accuracy: 0.5112 - loss: 0.6002 - val_accuracy: 0.4285 - val_loss: 1.4598 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.5166 - loss: 0.5716\n",
            "Epoch 16 translations:\n",
            "EN: show your own business .\n",
            "JA: 仕事 を どうぞ 。\n",
            "\n",
            "EN: he lived a hard life .\n",
            "JA: 彼 は 辛 い 人生 を 送 っ た 。\n",
            "\n",
            "EN: no . i 'm sorry , i 've got to go back early .\n",
            "JA: いいえ 、 私 は 早 く 行 く こと に な っ て き た 。\n",
            "\n",
            "EN: she wrote to me to come at once .\n",
            "JA: 彼女 は すぐ に 私 に 手紙 を くれ た 。\n",
            "\n",
            "EN: i can 't swim at all .\n",
            "JA: 私 は まったく 泳げ な い 。\n",
            "\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 151ms/step - accuracy: 0.5166 - loss: 0.5717 - val_accuracy: 0.4280 - val_loss: 1.4649 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 148ms/step - accuracy: 0.5205 - loss: 0.5487 - val_accuracy: 0.4298 - val_loss: 1.4674 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 148ms/step - accuracy: 0.5243 - loss: 0.5331 - val_accuracy: 0.4296 - val_loss: 1.4692 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 147ms/step - accuracy: 0.5259 - loss: 0.5208 - val_accuracy: 0.4288 - val_loss: 1.4695 - learning_rate: 6.2500e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 148ms/step - accuracy: 0.5269 - loss: 0.5105 - val_accuracy: 0.4304 - val_loss: 1.4739 - learning_rate: 6.2500e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.5295 - loss: 0.5082\n",
            "Epoch 21 translations:\n",
            "EN: show your own business .\n",
            "JA: 仕事 を [UNK] し て くださ い 。\n",
            "\n",
            "EN: he lived a hard life .\n",
            "JA: 彼 は 辛 い 人生 を 送 っ た 。\n",
            "\n",
            "EN: no . i 'm sorry , i 've got to go back early .\n",
            "JA: いいえ 、 もう 失礼 で す から 。\n",
            "\n",
            "EN: she wrote to me to come at once .\n",
            "JA: 彼女 は すぐ に 私 に 手紙 を くれ と くれ た 。\n",
            "\n",
            "EN: i can 't swim at all .\n",
            "JA: 私 は まったく 泳げ な い 。\n",
            "\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 152ms/step - accuracy: 0.5295 - loss: 0.5082 - val_accuracy: 0.4300 - val_loss: 1.4751 - learning_rate: 3.1250e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 147ms/step - accuracy: 0.5300 - loss: 0.5040 - val_accuracy: 0.4308 - val_loss: 1.4737 - learning_rate: 3.1250e-05\n",
            "\n",
            "Training complete! Best val_loss: 1.4520\n"
          ]
        }
      ]
    }
  ]
}