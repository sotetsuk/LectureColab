{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPfyT+D/pzN8xY16IQThLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sotetsuk/LectureColab/blob/main/torch_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cNhHqFB_tGO",
        "outputId": "2c46c682-e0f0-4a52-e9c5-2932fe556efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 2)\n",
            "(200,)\n",
            "Epoch [1/200], Loss: 9.2534, Accuracy: 50.00%\n",
            "Epoch [2/200], Loss: 8.9929, Accuracy: 50.00%\n",
            "Epoch [3/200], Loss: 8.8258, Accuracy: 56.00%\n",
            "Epoch [4/200], Loss: 8.6706, Accuracy: 65.00%\n",
            "Epoch [5/200], Loss: 8.5252, Accuracy: 64.50%\n",
            "Epoch [6/200], Loss: 8.3042, Accuracy: 68.00%\n",
            "Epoch [7/200], Loss: 8.0154, Accuracy: 71.00%\n",
            "Epoch [8/200], Loss: 7.7076, Accuracy: 70.00%\n",
            "Epoch [9/200], Loss: 7.2903, Accuracy: 75.00%\n",
            "Epoch [10/200], Loss: 6.8930, Accuracy: 79.00%\n",
            "Epoch [11/200], Loss: 6.2947, Accuracy: 85.00%\n",
            "Epoch [12/200], Loss: 5.7749, Accuracy: 87.50%\n",
            "Epoch [13/200], Loss: 5.2963, Accuracy: 88.00%\n",
            "Epoch [14/200], Loss: 4.8559, Accuracy: 91.50%\n",
            "Epoch [15/200], Loss: 4.4677, Accuracy: 90.50%\n",
            "Epoch [16/200], Loss: 3.9654, Accuracy: 95.50%\n",
            "Epoch [17/200], Loss: 3.5919, Accuracy: 96.50%\n",
            "Epoch [18/200], Loss: 3.3015, Accuracy: 94.50%\n",
            "Epoch [19/200], Loss: 3.0354, Accuracy: 96.50%\n",
            "Epoch [20/200], Loss: 2.9656, Accuracy: 95.50%\n",
            "Epoch [21/200], Loss: 2.8006, Accuracy: 95.00%\n",
            "Epoch [22/200], Loss: 2.5398, Accuracy: 96.00%\n",
            "Epoch [23/200], Loss: 2.3453, Accuracy: 95.00%\n",
            "Epoch [24/200], Loss: 2.3233, Accuracy: 95.00%\n",
            "Epoch [25/200], Loss: 2.3558, Accuracy: 96.00%\n",
            "Epoch [26/200], Loss: 2.1794, Accuracy: 96.50%\n",
            "Epoch [27/200], Loss: 2.1069, Accuracy: 92.50%\n",
            "Epoch [28/200], Loss: 2.0696, Accuracy: 95.50%\n",
            "Epoch [29/200], Loss: 2.1462, Accuracy: 94.00%\n",
            "Epoch [30/200], Loss: 2.1476, Accuracy: 95.50%\n",
            "Epoch [31/200], Loss: 2.0816, Accuracy: 95.00%\n",
            "Epoch [32/200], Loss: 2.0122, Accuracy: 95.50%\n",
            "Epoch [33/200], Loss: 1.7195, Accuracy: 96.50%\n",
            "Epoch [34/200], Loss: 1.7419, Accuracy: 96.50%\n",
            "Epoch [35/200], Loss: 1.7994, Accuracy: 96.50%\n",
            "Epoch [36/200], Loss: 1.6844, Accuracy: 96.50%\n",
            "Epoch [37/200], Loss: 1.6441, Accuracy: 96.50%\n",
            "Epoch [38/200], Loss: 1.6506, Accuracy: 96.00%\n",
            "Epoch [39/200], Loss: 1.6191, Accuracy: 96.50%\n",
            "Epoch [40/200], Loss: 1.5403, Accuracy: 96.00%\n",
            "Epoch [41/200], Loss: 1.6610, Accuracy: 95.00%\n",
            "Epoch [42/200], Loss: 1.7205, Accuracy: 95.50%\n",
            "Epoch [43/200], Loss: 1.6757, Accuracy: 96.50%\n",
            "Epoch [44/200], Loss: 1.5581, Accuracy: 95.50%\n",
            "Epoch [45/200], Loss: 1.5035, Accuracy: 96.00%\n",
            "Epoch [46/200], Loss: 1.5018, Accuracy: 95.50%\n",
            "Epoch [47/200], Loss: 1.5621, Accuracy: 96.00%\n",
            "Epoch [48/200], Loss: 1.5210, Accuracy: 94.00%\n",
            "Epoch [49/200], Loss: 1.6163, Accuracy: 95.50%\n",
            "Epoch [50/200], Loss: 1.4333, Accuracy: 97.00%\n",
            "Epoch [51/200], Loss: 1.5467, Accuracy: 95.00%\n",
            "Epoch [52/200], Loss: 1.4298, Accuracy: 96.50%\n",
            "Epoch [53/200], Loss: 1.4724, Accuracy: 96.00%\n",
            "Epoch [54/200], Loss: 1.6447, Accuracy: 96.00%\n",
            "Epoch [55/200], Loss: 1.5275, Accuracy: 95.00%\n",
            "Epoch [56/200], Loss: 1.6303, Accuracy: 96.00%\n",
            "Epoch [57/200], Loss: 1.3906, Accuracy: 96.00%\n",
            "Epoch [58/200], Loss: 1.5004, Accuracy: 95.50%\n",
            "Epoch [59/200], Loss: 1.5490, Accuracy: 96.50%\n",
            "Epoch [60/200], Loss: 1.6451, Accuracy: 95.00%\n",
            "Epoch [61/200], Loss: 1.5019, Accuracy: 96.00%\n",
            "Epoch [62/200], Loss: 1.6506, Accuracy: 94.50%\n",
            "Epoch [63/200], Loss: 1.3695, Accuracy: 96.00%\n",
            "Epoch [64/200], Loss: 1.6751, Accuracy: 96.00%\n",
            "Epoch [65/200], Loss: 1.5115, Accuracy: 95.00%\n",
            "Epoch [66/200], Loss: 1.3749, Accuracy: 96.50%\n",
            "Epoch [67/200], Loss: 1.4202, Accuracy: 96.00%\n",
            "Epoch [68/200], Loss: 1.3557, Accuracy: 96.00%\n",
            "Epoch [69/200], Loss: 1.3885, Accuracy: 96.00%\n",
            "Epoch [70/200], Loss: 1.5935, Accuracy: 95.50%\n",
            "Epoch [71/200], Loss: 1.5014, Accuracy: 95.00%\n",
            "Epoch [72/200], Loss: 1.3464, Accuracy: 97.00%\n",
            "Epoch [73/200], Loss: 1.5295, Accuracy: 96.50%\n",
            "Epoch [74/200], Loss: 1.5234, Accuracy: 96.00%\n",
            "Epoch [75/200], Loss: 1.5716, Accuracy: 96.00%\n",
            "Epoch [76/200], Loss: 2.0407, Accuracy: 95.00%\n",
            "Epoch [77/200], Loss: 1.5479, Accuracy: 96.00%\n",
            "Epoch [78/200], Loss: 1.4638, Accuracy: 95.50%\n",
            "Epoch [79/200], Loss: 1.3989, Accuracy: 96.50%\n",
            "Epoch [80/200], Loss: 1.4895, Accuracy: 96.00%\n",
            "Epoch [81/200], Loss: 1.3801, Accuracy: 96.50%\n",
            "Epoch [82/200], Loss: 1.4181, Accuracy: 96.00%\n",
            "Epoch [83/200], Loss: 1.3782, Accuracy: 95.50%\n",
            "Epoch [84/200], Loss: 1.4393, Accuracy: 96.00%\n",
            "Epoch [85/200], Loss: 1.3964, Accuracy: 95.50%\n",
            "Epoch [86/200], Loss: 1.3158, Accuracy: 96.50%\n",
            "Epoch [87/200], Loss: 1.4695, Accuracy: 96.50%\n",
            "Epoch [88/200], Loss: 1.3553, Accuracy: 96.00%\n",
            "Epoch [89/200], Loss: 1.3581, Accuracy: 95.50%\n",
            "Epoch [90/200], Loss: 1.3497, Accuracy: 96.50%\n",
            "Epoch [91/200], Loss: 1.4053, Accuracy: 96.00%\n",
            "Epoch [92/200], Loss: 1.3325, Accuracy: 96.50%\n",
            "Epoch [93/200], Loss: 1.3508, Accuracy: 96.50%\n",
            "Epoch [94/200], Loss: 1.5279, Accuracy: 96.50%\n",
            "Epoch [95/200], Loss: 1.3462, Accuracy: 96.00%\n",
            "Epoch [96/200], Loss: 1.3267, Accuracy: 96.00%\n",
            "Epoch [97/200], Loss: 1.3462, Accuracy: 96.00%\n",
            "Epoch [98/200], Loss: 1.3525, Accuracy: 96.50%\n",
            "Epoch [99/200], Loss: 1.3594, Accuracy: 96.00%\n",
            "Epoch [100/200], Loss: 1.4299, Accuracy: 95.50%\n",
            "Epoch [101/200], Loss: 1.3630, Accuracy: 96.50%\n",
            "Epoch [102/200], Loss: 1.3054, Accuracy: 96.50%\n",
            "Epoch [103/200], Loss: 1.3506, Accuracy: 95.50%\n",
            "Epoch [104/200], Loss: 1.5107, Accuracy: 96.00%\n",
            "Epoch [105/200], Loss: 1.5771, Accuracy: 95.50%\n",
            "Epoch [106/200], Loss: 1.6494, Accuracy: 94.50%\n",
            "Epoch [107/200], Loss: 1.6961, Accuracy: 96.50%\n",
            "Epoch [108/200], Loss: 1.4965, Accuracy: 95.50%\n",
            "Epoch [109/200], Loss: 1.4773, Accuracy: 94.50%\n",
            "Epoch [110/200], Loss: 1.3284, Accuracy: 96.00%\n",
            "Epoch [111/200], Loss: 1.5465, Accuracy: 96.00%\n",
            "Epoch [112/200], Loss: 1.2764, Accuracy: 96.50%\n",
            "Epoch [113/200], Loss: 1.3949, Accuracy: 94.50%\n",
            "Epoch [114/200], Loss: 1.3105, Accuracy: 96.00%\n",
            "Epoch [115/200], Loss: 1.3398, Accuracy: 96.00%\n",
            "Epoch [116/200], Loss: 1.3303, Accuracy: 96.00%\n",
            "Epoch [117/200], Loss: 1.3163, Accuracy: 96.50%\n",
            "Epoch [118/200], Loss: 1.3084, Accuracy: 96.00%\n",
            "Epoch [119/200], Loss: 1.3797, Accuracy: 96.00%\n",
            "Epoch [120/200], Loss: 1.5228, Accuracy: 96.00%\n",
            "Epoch [121/200], Loss: 1.6367, Accuracy: 94.50%\n",
            "Epoch [122/200], Loss: 1.5944, Accuracy: 95.00%\n",
            "Epoch [123/200], Loss: 1.3351, Accuracy: 96.00%\n",
            "Epoch [124/200], Loss: 1.3924, Accuracy: 96.00%\n",
            "Epoch [125/200], Loss: 1.5596, Accuracy: 96.00%\n",
            "Epoch [126/200], Loss: 1.4422, Accuracy: 95.50%\n",
            "Epoch [127/200], Loss: 1.4047, Accuracy: 96.50%\n",
            "Epoch [128/200], Loss: 1.5925, Accuracy: 96.50%\n",
            "Epoch [129/200], Loss: 1.4845, Accuracy: 95.00%\n",
            "Epoch [130/200], Loss: 1.3262, Accuracy: 96.50%\n",
            "Epoch [131/200], Loss: 1.2709, Accuracy: 96.00%\n",
            "Epoch [132/200], Loss: 1.3134, Accuracy: 96.50%\n",
            "Epoch [133/200], Loss: 1.3410, Accuracy: 96.00%\n",
            "Epoch [134/200], Loss: 1.2984, Accuracy: 96.50%\n",
            "Epoch [135/200], Loss: 1.3185, Accuracy: 96.00%\n",
            "Epoch [136/200], Loss: 1.3093, Accuracy: 97.00%\n",
            "Epoch [137/200], Loss: 1.2835, Accuracy: 96.50%\n",
            "Epoch [138/200], Loss: 1.3395, Accuracy: 96.00%\n",
            "Epoch [139/200], Loss: 1.2861, Accuracy: 96.00%\n",
            "Epoch [140/200], Loss: 1.3050, Accuracy: 96.00%\n",
            "Epoch [141/200], Loss: 1.3345, Accuracy: 96.00%\n",
            "Epoch [142/200], Loss: 1.3455, Accuracy: 96.00%\n",
            "Epoch [143/200], Loss: 1.5534, Accuracy: 96.50%\n",
            "Epoch [144/200], Loss: 1.4485, Accuracy: 95.50%\n",
            "Epoch [145/200], Loss: 1.3101, Accuracy: 97.00%\n",
            "Epoch [146/200], Loss: 1.2786, Accuracy: 96.00%\n",
            "Epoch [147/200], Loss: 1.5594, Accuracy: 96.00%\n",
            "Epoch [148/200], Loss: 1.8034, Accuracy: 96.00%\n",
            "Epoch [149/200], Loss: 1.5170, Accuracy: 96.00%\n",
            "Epoch [150/200], Loss: 1.3632, Accuracy: 96.00%\n",
            "Epoch [151/200], Loss: 1.3500, Accuracy: 96.00%\n",
            "Epoch [152/200], Loss: 1.6722, Accuracy: 97.00%\n",
            "Epoch [153/200], Loss: 1.4416, Accuracy: 95.00%\n",
            "Epoch [154/200], Loss: 1.6414, Accuracy: 95.50%\n",
            "Epoch [155/200], Loss: 1.2900, Accuracy: 97.00%\n",
            "Epoch [156/200], Loss: 1.3287, Accuracy: 96.00%\n",
            "Epoch [157/200], Loss: 1.2699, Accuracy: 96.00%\n",
            "Epoch [158/200], Loss: 1.2778, Accuracy: 96.00%\n",
            "Epoch [159/200], Loss: 1.2496, Accuracy: 96.00%\n",
            "Epoch [160/200], Loss: 1.3693, Accuracy: 95.50%\n",
            "Epoch [161/200], Loss: 1.4921, Accuracy: 95.50%\n",
            "Epoch [162/200], Loss: 1.3056, Accuracy: 96.00%\n",
            "Epoch [163/200], Loss: 1.2540, Accuracy: 96.50%\n",
            "Epoch [164/200], Loss: 1.4559, Accuracy: 96.00%\n",
            "Epoch [165/200], Loss: 1.3483, Accuracy: 96.00%\n",
            "Epoch [166/200], Loss: 1.3710, Accuracy: 97.00%\n",
            "Epoch [167/200], Loss: 1.6874, Accuracy: 96.50%\n",
            "Epoch [168/200], Loss: 1.3795, Accuracy: 96.00%\n",
            "Epoch [169/200], Loss: 1.3403, Accuracy: 97.00%\n",
            "Epoch [170/200], Loss: 1.2491, Accuracy: 96.50%\n",
            "Epoch [171/200], Loss: 1.3153, Accuracy: 95.00%\n",
            "Epoch [172/200], Loss: 1.3827, Accuracy: 96.00%\n",
            "Epoch [173/200], Loss: 1.4989, Accuracy: 97.00%\n",
            "Epoch [174/200], Loss: 1.3993, Accuracy: 96.00%\n",
            "Epoch [175/200], Loss: 1.3442, Accuracy: 96.50%\n",
            "Epoch [176/200], Loss: 1.2417, Accuracy: 96.50%\n",
            "Epoch [177/200], Loss: 1.2689, Accuracy: 96.00%\n",
            "Epoch [178/200], Loss: 1.4136, Accuracy: 96.50%\n",
            "Epoch [179/200], Loss: 1.2647, Accuracy: 96.50%\n",
            "Epoch [180/200], Loss: 1.2999, Accuracy: 96.00%\n",
            "Epoch [181/200], Loss: 1.2769, Accuracy: 96.00%\n",
            "Epoch [182/200], Loss: 1.3386, Accuracy: 96.50%\n",
            "Epoch [183/200], Loss: 1.3188, Accuracy: 96.00%\n",
            "Epoch [184/200], Loss: 1.3203, Accuracy: 96.00%\n",
            "Epoch [185/200], Loss: 1.3609, Accuracy: 96.00%\n",
            "Epoch [186/200], Loss: 1.3136, Accuracy: 96.00%\n",
            "Epoch [187/200], Loss: 1.3274, Accuracy: 96.00%\n",
            "Epoch [188/200], Loss: 1.3825, Accuracy: 96.00%\n",
            "Epoch [189/200], Loss: 1.3291, Accuracy: 96.50%\n",
            "Epoch [190/200], Loss: 1.3041, Accuracy: 96.00%\n",
            "Epoch [191/200], Loss: 1.3746, Accuracy: 95.50%\n",
            "Epoch [192/200], Loss: 1.3672, Accuracy: 96.50%\n",
            "Epoch [193/200], Loss: 1.6401, Accuracy: 97.50%\n",
            "Epoch [194/200], Loss: 1.4108, Accuracy: 94.50%\n",
            "Epoch [195/200], Loss: 1.4743, Accuracy: 96.00%\n",
            "Epoch [196/200], Loss: 1.3839, Accuracy: 96.50%\n",
            "Epoch [197/200], Loss: 1.7628, Accuracy: 96.00%\n",
            "Epoch [198/200], Loss: 1.4315, Accuracy: 96.50%\n",
            "Epoch [199/200], Loss: 1.4852, Accuracy: 96.00%\n",
            "Epoch [200/200], Loss: 1.2453, Accuracy: 96.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, y = make_circles(n_samples=200, noise=0.15, factor=0.4, random_state=42)\n",
        "print(X.shape)  # X: (200, 2) のデータ点 (特徴量 x1, x2)\n",
        "print(y.shape)  # y: (200,) のラベル (0 or 1)\n",
        "\n",
        "# NumPy配列をPyTorchテンソルに変換\n",
        "X = torch.FloatTensor(X)\n",
        "y = torch.FloatTensor(y).unsqueeze(1)  # (200, 1) の形状に変更\n",
        "\n",
        "# DataLoaderの作成\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),      # 隠れ層: 4ノード\n",
        "    nn.ReLU(),            # 活性化関数 ReLU\n",
        "    nn.Linear(4, 2),      # 隠れ層: 2ノード\n",
        "    nn.ReLU(),            # 活性化関数 ReLU\n",
        "    nn.Linear(2, 1),      # 出力層: 1ノード (2クラス分類)\n",
        "    nn.Sigmoid()          # 活性化関数 Sigmoid\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 訓練ループ\n",
        "for epoch in range(200):\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Train Accuracy\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/200], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%')"
      ]
    }
  ]
}